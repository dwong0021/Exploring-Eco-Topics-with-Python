{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94c029c2-0951-43c4-b474-95e19a0c0545",
   "metadata": {},
   "source": [
    "# Carbon Polluters Exploration with Python\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1debba17-0daf-483e-b91b-2a18520d60b2",
   "metadata": {},
   "source": [
    "**<font color='red'>FYI internet access required for Section E.</font>**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386bb7ed-6b5f-4cc5-98c6-fdcec098f1c9",
   "metadata": {},
   "source": [
    "**Which sample of Carbon Polluters are we examining?**    \n",
    "US university facilities with large GHG emissions (> 25,000 metric tons of carbon dioxide equivalent (CO 2 e) per year) in any year between 2011 to 2021 (the latest reporting year). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3266731b-fd74-4db5-8f68-fe285163fdea",
   "metadata": {},
   "source": [
    "**What scientific approaches are we taking?**    \n",
    "Statistical and geospatial approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6569c0-6701-4075-8c8d-985e80835406",
   "metadata": {},
   "source": [
    "**What outputs will we develop?**    \n",
    "Statistical graphs and interactive maps, with historical and/or regional dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0192dc-0067-43a5-b8a1-81d83545778b",
   "metadata": {},
   "source": [
    "**What will our outputs tell us?**    \n",
    "Who and where are the significant US sources of carbon pollution within the Higher Education sector, at facility and state level, both recent and since 2011.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddb4bce-8f1c-489a-82e8-9bf3e2c31825",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Beyond the well-known Eco impacts of Carbon Polluters, what makes this sample significant?**    \n",
    "The fact that this sector has/individual universities are large fossil fuel burners may be a surprise seemingly out of sync with any green credentials or reputation they have garnered, especially relating to clean energy.\n",
    "    \n",
    "For further/corroborating findings, see recent Reuters article -> https://www.reuters.com/investigates/special-report/usa-pollution-universities/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b99fc0-d779-4f7c-8a68-d36fb1f8534f",
   "metadata": {},
   "source": [
    "---\n",
    "**Data Source - University Emitters**\n",
    "* Tutorial file: `CarbonPollutersExp_DATA.csv`\n",
    "* Org: U.S. Environmental Protection Agency (EPA)\n",
    "* Resource:  Facility Level Information on GreenHouse gases Tool (FLIGHT), which provides information about greenhouse gas (GHG) emissions from large facilities in the U.S., who are required to report annual data about their GHG emissions to EPA as part of the Greenhouse Gas Reporting Program (GHGRP) -> https://ghgdata.epa.gov/ghgp/main.do\n",
    "* Related resources: All GHGRP data products -> https://www.epa.gov/ghgreporting/find-and-use-ghgrp-data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d26caeb-e419-408f-a7f7-e95f368eb962",
   "metadata": {},
   "source": [
    "**Data Source - US state shapefile**\n",
    "* Tutorial subfolder: `cb_2021_us_state_20m`\n",
    "* Org: U.S. Census Bureau \n",
    "* Resource: Cartographic boundary files -> https://www.census.gov/geographies/mapping-files/time-series/geo/cartographic-boundary.html\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f1cd6b-6556-4653-b896-da195a7952a1",
   "metadata": {},
   "source": [
    "## A. Set-up Jupyter Notebook, University Emitters dataset & US state shapefile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df048f95-f24c-4cfe-8044-2310b3fbc779",
   "metadata": {},
   "source": [
    ">**A0.** Import the required packages and submodule with their conventional aliases.\n",
    ">```\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9687f612-9ada-49dd-ae3e-70ebc3ec0a61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "937b1b5e-09aa-419c-aaf5-9f9ce7305e94",
   "metadata": {},
   "source": [
    ">**A1.** (OPTIONAL) For autocompletion, or if it's not working, try running this magic command.\n",
    ">```\n",
    "%config Completer.use_jedi = False\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259507ec-f759-4a7c-b355-7db6b4e9baef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "311e23ca-39ef-45b6-bff1-b2d1ce4fdd46",
   "metadata": {},
   "source": [
    ">**A2.** Set pandas display options to show all columns/not truncate their display.\n",
    ">```\n",
    "pd.options.display.max_columns = None\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a778d7-cf47-4ddd-9209-759a002feacd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06e12de9-0aec-4b71-843b-be6765b410de",
   "metadata": {},
   "source": [
    ">**A3.** Read-in the university emitters dataset `\"CarbonPollutersExp_DATA\"`, and assign to `raw_data`.    \n",
    ">\n",
    ">**Code Detail:** Although we are still passing in just the path to the file as the only required parameter of the `read_csv()` method, note from the Docstring the range of options available for customising the call.\n",
    ">```\n",
    "raw_data = pd.read_csv(\"CarbonPollutersExp_DATA.csv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45f8ffc-918d-4dd4-9bd0-429fc2af6e8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2beba175-7144-4e3b-946e-f12b550329fb",
   "metadata": {},
   "source": [
    ">**A4.** Copy `raw_data` to create the `DataFrame` we will be prepping called `df_prep`.\n",
    ">```\n",
    "df_prep = raw_data.copy()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f6813b-ac4f-4b38-928d-61604ba404d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec1d740c-337b-4eea-ab2d-fad90fb7f852",
   "metadata": {},
   "source": [
    ">**A5.** Ensure the US state shapefile `cb_2021_us_state_20m` is a subfolder in your current working directory (or locally accessible).  \n",
    "> * This shapefile is an Esri vector data storage format that stores the official location, shape, and attributes for each state at the 1 : 20,000,000 (national) ratio scale. \n",
    "> * `cb_2021_us_state_20m` contains a set of related files, such as the `.shp`, `.shx`, `.dbf`, and `.prj` files components of the shapefile.\n",
    "> * Download here -> https://www2.census.gov/geo/tiger/GENZ2021/shp/cb_2021_us_state_20m.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35338cf6-8c4a-4a29-a37f-04c2dec2ebdf",
   "metadata": {},
   "source": [
    "---\n",
    "## B. Inspect the University Emitters dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568182bb-7e8e-4df7-b3c7-af94418bd841",
   "metadata": {},
   "source": [
    ">**B0.** Have a look at the dataset of large carbon polluting US university facilities.\n",
    ">\n",
    ">**Code Detail:** Instead of `head()` returning a default number of rows (5), start specifying the `n` argument.\n",
    ">```\n",
    "df_prep.head(2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3067d89f-88ac-47d3-ae9d-79983d6e240f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6f785da-dbc6-4b4a-8472-709cd88f1184",
   "metadata": {},
   "source": [
    "<font color=\"green\">***B0. Comments***    \n",
    "*- Like the original Pangolin Trade data, there's missing data we need to handle.*    \n",
    "*- However, unlike the Pangolin Trade data, the data requested from the EPA FLIGHT data source is the exact sample of interest, namely all facilities in the US universities sector with large emissions in any reporting year between 2011 and 2021. Every row is therefore relevant.*    \n",
    "*- The required data prep (also called cleaning/munging/wrangling) is, therefore, needed to modify the columns, not the rows, which is the focus of the next section C.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ce821f-5d5c-45d4-8020-ba04efee00f9",
   "metadata": {},
   "source": [
    ">**B1.** Find out the dimensions of `df_prep`.\n",
    ">```\n",
    "df_prep.shape\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345c0fcc-cf8c-4e49-ab6a-377f13e63e79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31e57737-cb85-40ba-843a-7ed8c88df7d5",
   "metadata": {},
   "source": [
    "<font color=\"green\">***B1. Comment***     \n",
    "*- There are 118 US university facilities which were large carbon polluters in at least one of the years between 2011-2021.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189cb53f-aa5a-4dfb-ad27-e8ebefc99e56",
   "metadata": {},
   "source": [
    "---\n",
    "## C. Prepare the University Emitters dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980610d2-b137-4eee-91ab-42f91a319c6f",
   "metadata": {},
   "source": [
    ">**C0.** Some of the columns are definitely not needed, so let's prepare to drop them, and with some efficiency. Instead of manually counting, let's programmatically find out the index positions of each `df_prep` column.\n",
    ">```\n",
    "list(enumerate(df_prep.columns))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b113f3-200a-4391-9ec1-333dee5d339e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c8ee854-3176-436b-b038-ebc2a5ff9a01",
   "metadata": {},
   "source": [
    ">**C1.** Perform a drop operation that removes 4 particular columns from `df_prep` inplace (`SUBPARTS`, `CHANGE IN EMISSIONS (2020 TO 2021)`, `CHANGE IN EMISSIONS (2011 TO 2021)`, and `SECTORS`). Review the modification.    \n",
    ">\n",
    ">**Code Detail:** Pass the relevant subset of the `df_prep.columns` attribute as the named `columns` argument of the `drop()` method.    \n",
    ">\n",
    ">**Tech Note:** The `inplace` parameter is accepted by several pandas methods is used in this Tutorial where possible for brevity, but it's existence is controversial and currently in flux.\n",
    ">```\n",
    "df_prep.drop(columns = df_prep.columns[[10,22,23,24]], inplace=True)\n",
    "df_prep.head(2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8066053a-0410-411f-a3e1-d250c3260cfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f50600d-1ef0-431f-a8f2-0ff9c8074646",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e96d1fc-55e4-44af-8591-baecf89bc38d",
   "metadata": {},
   "source": [
    ">**C2.** See what `dtype` pandas inferred was in each column when it originally read-in the `CarbonPollutersExp_DATA.csv`.\n",
    ">```\n",
    "df_prep.dtypes\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5900f05-b744-49ad-b66f-b2aabc0f9f8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "674839d7-0a42-40ac-8352-5c397d4b995a",
   "metadata": {},
   "source": [
    "<font color=\"green\">***C2. Comment***     \n",
    "*- pandas' inferences are only partial accurate. The columns that are essential to correct are the 11 years of reported emissions data. These are all currently `object` `dtype`, so basically string data, not numeric as required.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b320f1-1cd2-42ab-b522-1f9524f4135d",
   "metadata": {},
   "source": [
    ">**C3.** Before we start modifying the 11 reported emissions data columns, let's shorten their long, cumbersome labels to just the reporting year reference. Review the modification.    \n",
    ">\n",
    ">**Code Detail:** Perform a renaming operation inplace where a `replace()` string method that returns a copy of the original string with the substring `\"TOTAL REPORTED EMISSIONS, \"` replaced by nothing is applied to each column label of `df_prep`.\n",
    ">```\n",
    "df_prep.rename(columns = lambda x: x.replace(\"TOTAL REPORTED EMISSIONS, \", \"\"), inplace=True)\n",
    "df_prep.head(2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ea516f-b165-48d0-9c2e-080f8b54c7e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4b94be-e7d2-4180-bbea-b2519e4198f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d99bb17-5e42-4c51-93bc-ea8d44a72d4c",
   "metadata": {},
   "source": [
    ">**C4.** Now start the process towards converting these 11 columns to a numeric `dtype`. Remove the comma characters from the string data in these 11 columns, but not anywhere else in `df_prep` (e.g. `\"REPORTED ADDRESS\"`). Review the modification.\n",
    "> \n",
    ">**Code Detail:** Use pandas `loc` indexer to select these 11 `df_prep` columns by inputting a slice object with labels after the comma. `applymap()` applies a function to a Dataframe elementwise, as opposed to row/column-wise.    \n",
    ">```\n",
    "df_prep.loc[:, \"2011\":] = df_prep.loc[:, \"2011\":].applymap(lambda x: x.replace(\",\", \"\"))\n",
    "df_prep.head(2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a22672-41ec-4a1b-bca0-49664d400d70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b2baa6-3180-4637-8ddb-3f7e2e9768e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9626cb1a-09dc-4670-9fcb-e90ae870a54a",
   "metadata": {},
   "source": [
    "<font color=\"green\">***C4. Comment***        \n",
    "*- As a specific example of the general point that there are typically **multiple ways to do the same thing in scientific Python**, other ways we can select these 11 priority columns include:*\n",
    "```\n",
    "df_prep.iloc[:, -11:]\n",
    "df_prep[df_prep.columns[-11:]]\n",
    "df_prep.filter(regex=\"^20\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e267a0-6da0-4c09-8f54-cba7d5d03ab8",
   "metadata": {},
   "source": [
    ">**C5.** The last string clean-up step is to deal with the `\"---\"` instances in the 11 columns that we assume is EPA notation for `N/A`. We will replace these string values inplace with `NaN`. Review the modification.\n",
    ">```\n",
    "df_prep.replace(\"---\", np.nan, inplace=True)\n",
    "df_prep.head(2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7d79fc-0ce4-4c9b-89cd-b3f790ec0880",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c905fcfb-c84f-4c43-957d-58585bc93762",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7adcb16f-cd72-4178-bfdf-eaedcefc8091",
   "metadata": {},
   "source": [
    ">**C6.** Now convert the 11 columns to a numeric `dtype` (note the `DeprecationWarning`). Review the modification by eye.  \n",
    ">\n",
    ">**Code Detail:** An alternative right-hand expression is `df_prep.loc[:, \"2011\":].astype(float)`\n",
    ">```\n",
    "df_prep.loc[:, \"2011\":] = df_prep.loc[:, \"2011\":].apply(pd.to_numeric)\n",
    "df_prep.head(2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e9bf6f-4454-4b80-ac5e-fd2c80fb8870",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdb29c4-f367-4b1e-b926-d9ffb909a81e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4177dc36-884a-4f6d-96d7-620d64993e0b",
   "metadata": {},
   "source": [
    ">**C7.** Now review the modification more formally by accessing the `dtypes` attribute again.\n",
    ">```\n",
    "df_prep.dtypes\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aafcef2-85b0-4103-a562-f39dee4c29be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d657b76-4d4e-4ea6-88c7-8f9527623035",
   "metadata": {},
   "source": [
    ">**C8.** Now that the columns with the 11 years of reported emissions data are a numeric `dtype` (as well as some other columns), let's compute some quick summary statistics.    \n",
    ">\n",
    ">**Tech Note:** To show less/no decimal places use `pd.set_option (\"display.precision\", 0)`\n",
    ">```\n",
    "df_prep.describe()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae5c170-4306-400b-aaec-22f998d36b8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98b46667-ec26-45b0-9c40-ec27c6bd6632",
   "metadata": {},
   "source": [
    ">**C9.** The subset of 11 emissions columns is time series data. Generate a quick matplotlib line plot.\n",
    ">\n",
    ">**Code Detail:** Access the `T` attribute of the `df_prep` subset to return the transpose, then call the `plot()` method, using the `legend` keyword to not place a legend on the plot.  \n",
    ">\n",
    ">**Tech Note:** The `plot()` method for `DataFrame` or `Series` data structures uses the backend specified by the option `plotting.backend`, which has the default value of matplotlib.\n",
    ">```\n",
    "df_prep[ df_prep.columns[-11:]].T.plot(legend=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac203436-5ac0-45f4-80f8-e8a2fe3ccaa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2b9c9f3-aa6a-4439-9d24-01d3ef86092b",
   "metadata": {},
   "source": [
    "<font color=\"green\">***C9. Comment***    \n",
    "*- By eye it looks like all data points in the time series are above the 25,000 metric ton CO 2 e threshold as specified in the original EPA FLIGHT data request, but a programmatic check can optionally be performed in **C10.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef600816-c556-424a-856c-cb5674bad731",
   "metadata": {},
   "source": [
    ">**C10.** (OPTIONAL) Check that all the data points in the time series are either greater than 25,000 or `NaN` by generating a Boolean array for the conditions then determining whether all the values are `True` or not.\n",
    ">```\n",
    "( (df_prep.iloc[:, -11:] > 25000) | (df_prep.iloc[:, -11:].isna()) ).values.all()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b63b03-900d-4c7a-bf11-3f92a72926f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9f267dc-81d3-4e6a-8e48-1f021431133c",
   "metadata": {
    "tags": []
   },
   "source": [
    ">**C11.** Final task for **Section C.**, create a new `\"Cumulative\"` column with the sum of each facility's reported emissions data over the 11 years. Review the modification.\n",
    ">\n",
    ">**Code Detail:** Extend `df_prep` by assigning a new index value, `\"Cumulative\"` using the indexing operator. To perform the `sum()` operation on the columns axis a named `axis` argument of either `columns` or `1` must be passed.\n",
    ">```\n",
    "df_prep[\"Cumulative\"] = df_prep.loc[:, \"2011\":].sum(axis=1)\n",
    "df_prep.head(2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa22fb1e-a4c9-465e-9764-a3814c00779d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57eabadb-96e1-44e0-b030-cc90a1510153",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2134804-a9fb-41f3-9df5-f3232763c23f",
   "metadata": {},
   "source": [
    ">**C12.** (OPTIONAL) Calculate the total GHG emissions/volume of carbon pollution that the large facilities in the US university sector have been responsible for creating over the reported 11-year period.\n",
    ">```\n",
    "df_prep[\"Cumulative\"].sum()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ccc432-77b7-4cd1-97ca-b9db23ea37fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73e0608c-6421-45b6-9d2b-bcacaee7e076",
   "metadata": {},
   "source": [
    "<font color=\"green\">***C12. Comment***     \n",
    "*- These 118 US university facilities are responsible for ~96 million tons of carbon pollution from 2011 to 2021.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e6f703-812a-4fff-8ac7-e2c84f269f54",
   "metadata": {},
   "source": [
    "---\n",
    "## D. Map dataset for regional trends - Basic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e49c30-65f2-440d-87b1-6683add1477a",
   "metadata": {},
   "source": [
    "<font color=\"green\">***D. Intro***       \n",
    "*- For this mapping section D. and E. we use the geographic pandas extension, geopandas, to create a `GeoDataFrame` object.*       \n",
    "*- A `GeoDataFrame` is a pandas `DataFrame` that has a column with geometry, and extends pandas functionality in order to make basic maps.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ec5d64-0452-4349-a866-24da234f9034",
   "metadata": {},
   "source": [
    ">**D0.** Import geopandas with it's conventional alias.\n",
    ">```\n",
    "import geopandas as gpd\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72466e5-1b98-4a97-ab5f-41e27360543c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d86f4012-10f6-42e6-a850-b534e45d76fa",
   "metadata": {},
   "source": [
    ">**D1.** Create a `GeoDataFrame` called `geo_df` using `df_prep`. Review the new object.\n",
    "> \n",
    ">**Code Detail:** Call geopandas `GeoDataFrame()` function, inputting `df_prep` as well as a `geometry` keyword argument which is another geopandas function `points_from_xy()` called with `df_prep`'s `\"LONGITUDE\"` and `\"LATITUDE\"` columns as the required positional `x` and `y` arguments respectively.    \n",
    ">```\n",
    "geo_df = gpd.GeoDataFrame(df_prep, geometry=gpd.points_from_xy(df_prep[\"LONGITUDE\"], df_prep[\"LATITUDE\"]))\n",
    "geo_df.head()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a5728c-f335-4ad6-b86d-54e63e48c88f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529f34a0-4c64-4275-a9ca-bef19d7cb65d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a6cf05b-5ddb-494f-8889-3213a5ca8908",
   "metadata": {},
   "source": [
    ">**D2.** Generate a default plot of this new `GeoDataFrame` `geo_df`. Then try a customised plot where the colour of the points is based on their `\"Cumulative\"` column value. Finally try customising the colormap, `cmap`, used that reflect the `\"Cumulative\"` values.\n",
    "> \n",
    ">**Tech Note:** geopandas uses matplotlib for this `plot()` method. matplotlib has a range of built-in colormaps -> https://matplotlib.org/stable/tutorials/colors/colormaps.html\n",
    ">```\n",
    "geo_df.plot()\n",
    "geo_df.plot(column=\"Cumulative\")\n",
    "geo_df.plot(column=\"Cumulative\", cmap=\"cool\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a443b883-9184-4d42-85fe-9af8025483fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7995608-ed7d-4ba7-bd47-1264b49331ce",
   "metadata": {},
   "source": [
    ">**D3.** Evidently, `geo_df` plot needs a base map to contextualise the points. Load one of geopandas' available datasets, `\"naturalearth_lowres\"`, assign to `base_map`, then plot. \n",
    ">\n",
    ">**Tech Note:** `base_map` is also a `GeoDataFrame` object.\n",
    ">```\n",
    "base_map = gpd.read_file(gpd.datasets.get_path(\"naturalearth_lowres\")) \n",
    "base_map.plot()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7b7474-47e6-4064-9e66-aa3171281032",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f4ba0c-b733-4170-b1f7-5edbfe0113e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bbcf4dda-4ce8-4780-8f8b-34ea5a226a12",
   "metadata": {},
   "source": [
    ">**D4.** (OPTIONAL): To get a better sense of what the geometry data represents and enables have a quick look at some rows of `base_map`, which is just another `GeoDataFrame`, and also try accessing the `\"geometry\"` value for the USA in the data to see how it is literally a drawing made up of multiple polygons (whilst other countries may be one polygon). Learn more about vector data -> https://datacarpentry.org/organization-geospatial/02-intro-vector-data/  \n",
    ">\n",
    ">**Tech Note:** To view more of the `\"geometry\"` column change the pandas column width setting: `pd.set_option(\"display.max_colwidth\", None)`\n",
    ">```\n",
    "base_map.head()\n",
    "base_map.at[4, \"geometry\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2551a4-efbb-437f-80ed-c4d035c62807",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44351a91-b708-4674-8240-de64bedaafd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e41f300-a725-4994-b825-f7c6cdad9de3",
   "metadata": {},
   "source": [
    ">**D5.** (OPTIONAL) Find out the Coordinate Reference System (CRS) of the `base_map`.  \n",
    ">\n",
    ">**Tech Note:** From geopandas docs, \"The Coordinate Reference System (CRS) is important because the geometric shapes in a GeoSeries or GeoDataFrame object are simply a collection of coordinates in an arbitrary space. A CRS tells Python how those coordinates relate to places on the Earth.\" -> https://geopandas.org/en/stable/docs/user_guide/projections.html\n",
    ">```\n",
    "base_map.crs\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dac3c2a-51fb-4e0f-938b-22218861b2c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce8eb6b9-7fc8-4a10-b8dc-bf86ddc4a29c",
   "metadata": {},
   "source": [
    ">**D6.** Use matplotlib to construct a basic map that plots `geo_df` and `base_map` together, visualising the location of the 118 large university emitters and the relative size of their cumulative emissions over 2011-2021. \n",
    ">\n",
    ">**Tech Note:** Many matplotlib plotting routines start with `fig, ax = plt.subplots()`, even when the output is a single plot, as a `Figure` and an `Axes` object is created in one step. `Axes` is not an `axis` reference, rather an instance of the class `plt.Axes` that is a bounding box object with ticks and labels. Conventionally, `ax` is used to refer to an individual axes instance, or a group of axes instances...!!\n",
    ">```\n",
    "fig, ax = plt.subplots(figsize=(12,8))    # Create an empty matplotlib Figure and Axes  \n",
    "base_map.plot(ax=ax)\n",
    "geo_df.plot(ax=ax, column=\"Cumulative\", cmap=\"autumn_r\")    # Other input options include legend=True, legend_kwds={\"orientation\": \"horizontal\"}\n",
    "#plt.tight_layout()\n",
    "#plt.savefig(\"<SomeFilename>.png\", dpi=600)    # Save the Figure/Axes using matplotlib - use the optional dpi (dots-per-inch) argument to control the resolution of the png\n",
    "plt.show()    # Display plot\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcedc51-800a-4e36-a74a-4df6eeecbe22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10607502-d670-4e9e-a0f3-adcfc3ab4190",
   "metadata": {},
   "source": [
    "---\n",
    "## E. Map dataset for regional trends - Interactive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cddc1c2-30c9-4174-a6e8-826050623640",
   "metadata": {},
   "source": [
    "<font color=\"green\">***E. Intro***    \n",
    "*- In this section we extend our geopandas extension of pandas with folium, an optional geopandas dependency which provides interactive mapping from the Open-Source leaflet.js library via a Python interface.*         \n",
    "*- See geopandas docs -> https://geopandas.org/en/stable/gallery/plotting_with_folium.html*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e5f5fa-23f3-464d-b76c-37521a75b2fa",
   "metadata": {},
   "source": [
    ">**E0.** Import folium.\n",
    ">```\n",
    "import folium\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe93ecc0-fff0-42f8-ab7e-8898481175c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "450a482a-0c80-46c5-8223-7bda52c8b728",
   "metadata": {},
   "source": [
    ">**E1.** Test out the extended geopandas functionality that folium now enables. Call `explore()` on `base_map`.\n",
    ">\n",
    ">**Tech Node:** The leaflet/folium maps takes a moment to render.\n",
    ">```\n",
    "base_map.explore()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20255fae-134f-49af-80d9-c610e793f101",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7878f0a-0bca-4962-9c62-e0f4e8c7fd33",
   "metadata": {},
   "source": [
    ">**E2.** Rather than using a world map as a base layer to plot US-only data, let's prepare a regional map. Read-in the US Census states shapefile we double-checked in **Section A.**, creating another `GeoDataFrame`, and assign to `US_state_boundaries`.    \n",
    ">\n",
    ">**Tech Note:** Leave the `\"cb_2018_us_state_20m\"` directory and files as-is, they contain dependencies.\n",
    ">```\n",
    "US_state_boundaries = gpd.read_file(\"cb_2021_us_state_20m/cb_2021_us_state_20m.shp\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704f3c2c-b5cc-47b4-884b-bab295443f8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5cc6c788-9cd2-48f5-b938-e48a5c18fdb6",
   "metadata": {},
   "source": [
    ">**E3.** (OPTIONAL) `US_state_boundaries` is another `GeoDataFrame` like `base_map` with multi/polygons, although for US states rather than world countries. What folium now allows us to do is visualize `GeoDataFrame` data on a leaflet map. Try calling `explore()` again.    \n",
    ">```\n",
    "US_state_boundaries.explore()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ca37e6-0362-4433-83f7-faf5c0831adb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f8f7f8-bc28-4a71-8600-df411ab4f7d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09f0d929-c79d-43ec-8410-823246dc470a",
   "metadata": {},
   "source": [
    ">**E4.** Through a plotting routine harnessing folium-extended capabilities of geopandas with `geo_df` and `US_state_boundaries`, construct an improved version of our **D6.** basic map, namely an interactive map that visualises the location of the 118 large university emitters and the relative size of their cumulative emissions over 2011-2021.\n",
    ">\n",
    ">**Code Details:** \n",
    ">* Create a `base_layer` which is an `US_state_boundaries.explore()` object, but setting several optional arguments that adjust formatting and interactivity.    \n",
    ">* Then call `explore()` on `geo_df`, specifying this plot to be drawn on the existing map instance, `base_layer`, using the `m` keyword and `\"Cumulative\"` as the column to plot. Use available keyword arguments to fine-tune formatting and interactivity.\n",
    ">* Finally, display `base_layer`.\n",
    ">\n",
    ">**Tech Notes:**\n",
    "> * The `radius` of 4,828 meters set in `marker_kwds` is significant because this is the value used in EPA material on GHGRP facilities and their impact on surrounding communities, namely 3 miles -> https://edap.epa.gov/public/extensions/GHGRP-Demographic-Data-Highlights/GHGRP-Demographic-Data-Highlights.html\n",
    "> * min_zoom refers to how far out (smaller int), max zoom refers to how far in (larger int)\n",
    ">```\n",
    "base_layer = US_state_boundaries.explore(location=[39, -97], width=\"60%\", height=\"60%\", zoom_start=4, min_zoom=3, tooltip=False, style_kwds=dict(fillOpacity=1, weight=1, fillColor=\"gainsboro\"), highlight_kwds=dict(fillOpacity=0, weight=3))\n",
    ">\n",
    ">geo_df.explore(m=base_layer, column=\"Cumulative\", marker_type=\"circle\", marker_kwds=dict(radius=4828, fill=False), cmap=\"autumn_r\", popup=[\"FACILITY\", \"Cumulative\"], tooltip=False)\n",
    ">\n",
    ">#base_layer.save(\"<SomeFilename>.html\")    # Save map, e.g. as html\n",
    "base_layer\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5d1902-6885-4444-98ef-97e24702b1df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "404cc1fd-349d-48a0-8352-487eeb506e22",
   "metadata": {},
   "source": [
    ">**E5.** (OPTIONAL) Create folium heatmap plot to visualise the density of these facilities.\n",
    ">\n",
    ">**Code Detail:** The folium `HeatMap` needs a list of the points to plot in the form `[lat, long, weight (optional)]`. The `heatmap_points` list here is creating with list comprehension.\n",
    ">```\n",
    "from folium.plugins import HeatMap\n",
    ">\n",
    ">heatmap_points = [ [lat, long, weight] for lat, long, weight in zip(geo_df[\"LATITUDE\"], geo_df[\"LONGITUDE\"], geo_df[\"Cumulative\"]) ]\n",
    ">\n",
    ">map_layer = folium.Map(location=[39, -97], tiles=\"cartodbpositron\", width=\"60%\", height=\"60%\", zoom_start=4, min_zoom=3)\n",
    "HeatMap(heatmap_points, radius=15, blur=5).add_to(map_layer)\n",
    "#map_layer.save(\"<SomeFilename>.html\")\n",
    ">\n",
    ">map_layer\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a986bc6-8f6a-451d-8c4b-0b385712ef07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc224d46-3ff8-4837-a13a-bf0ad3395006",
   "metadata": {},
   "source": [
    "---\n",
    "## F. Plot dataset for statistical trends"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017c290c-2ed0-4182-badf-049fe9202e79",
   "metadata": {},
   "source": [
    "> **F0.** Copy `df_prep` and assign to `df_trends`, which we will prep for developing statistical graphs. Have a look at the new `DataFrame`.\n",
    ">```\n",
    "df_trends = df_prep.copy()\n",
    "df_trends.head(2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412e6d24-25c6-4c9c-bea7-56e94e5d8b12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42e78c7-080f-4caf-85f8-bc6e5f612320",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be075007-f7dc-4aa7-8f54-18b42587f1b5",
   "metadata": {},
   "source": [
    "> **F1.** Delete the unnecessary `\"geometry\"` column.\n",
    ">```\n",
    "del df_trends[\"geometry\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6f7282-d2e7-477c-9ba2-c673f70af76a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "882c9a5d-655f-4a4d-8a30-4fe0b094f599",
   "metadata": {},
   "source": [
    ">**F2.** The statistical graphs we're developing plot each facility separately, and also separately within State subsets. It's now useful for the row labels to be meaningful, rather than the integer index created by default by the original pandas `read_csv()` call. Set the index of `df_trends` using the existing columns `\"FACILITY\"` and `\"STATE\"`, making the change inplace. Review the modification.\n",
    ">\n",
    ">**Tech Note:** `df_trends` now has a `MultiIndex`.\n",
    ">```\n",
    "df_trends.set_index([\"FACILITY\", \"STATE\"], inplace=True)\n",
    "df_trends.head(2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa49f2c-990e-474d-a2ad-530bc41cfbd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b78f2c3-5358-472b-b4f6-f4c01ba981b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cbabb7ef-7f5c-43e2-b8fc-9abd31661af8",
   "metadata": {},
   "source": [
    ">**F3.** The statistical graphs we're developing only need the columns with stats data (now that the essential non-stats data is now the row label). Drop the slice of columns from `\"GHGRP ID\"` to `\"PARENT COMPANIES\"` inplace. Review the modification.\n",
    ">```\n",
    "df_trends.drop(df_trends.loc[:, \"GHGRP ID\":\"PARENT COMPANIES\"], axis=1, inplace=True)\n",
    "df_trends.head(2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53b8d9a-f078-4800-bb2c-4d6e77e87b6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eeabfd4-86e9-4316-b7a8-b4ea5c2ba929",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29b5b1a1-f9b8-4b9f-928d-403d1bd9520f",
   "metadata": {},
   "source": [
    ">**F4.** Generate one of the target statistical graphs, namely a horizontal stacked bar chart of each facility's reported year emissions, ordering the facilities by their `\"Cumulative\"` value.\n",
    ">\n",
    ">**Code Detail:** Accept `sort_values()` default behaviour to sort ascending, as the horizontal bar chart plots from the origin of the y-axis up, so the facilities with the largest `\"Cumulative\"` values will be top, as desired.\n",
    ">\n",
    ">**Tech Note:** The computation takes a moment!\n",
    ">```\n",
    "df_trends.sort_values(\"Cumulative\").loc[:, \"2011\":\"2021\"].plot(kind=\"barh\", stacked=True, figsize=(20,15), legend=False, fontsize=8)   # To save the plot and with 600 dpi, chain the following: .get_figure().savefig(\"<SomeFilename>.png\", dpi=600)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363feb20-ac94-4b0f-81fc-82a423c25735",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6784333f-2226-4598-9bbf-923b03324482",
   "metadata": {},
   "source": [
    ">**F5.** Prepare for generating the final statistical graph, visualising the time series of each facility but in subplots by state, and only for the 12 states with the worst aggregate cumulative emissions. Compute the state-level aggregate cumulative emissions.\n",
    ">```\n",
    "df_trends.groupby(\"STATE\")[\"Cumulative\"].sum()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65da92a-8ff9-45b8-a832-6b9818182793",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f593994-02e3-4213-9152-1e39b468770b",
   "metadata": {},
   "source": [
    ">**F6.** Re-use the code from **F5.** but this time sort the default alphabetical output from worst to best, access just the first 12 indices. Assign this expression to `worst12states` and review.\n",
    "```\n",
    "worst12states = df_trends.groupby([\"STATE\"])[\"Cumulative\"].sum().sort_values(ascending=False).index[:12]\n",
    "worst12states\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdcf9e2-e62d-4546-9aa0-5d9a1fb818dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffc0f9a-0b52-493a-97ef-cbd34eeb632d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bda33035-8e7d-48f5-b346-6487eca26972",
   "metadata": {},
   "source": [
    ">**F7.** Assign a copy of `df_trends` without the `\"Cumulative\"` column to a variable `df_timeseries`, and review.\n",
    ">```\n",
    "df_timeseries = df_trends.drop(columns = \"Cumulative\").copy()\n",
    "df_timeseries.head(2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56950b1-5d24-43f5-bb05-0ab40cf43d59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be13620b-d825-4561-8789-e25966d4de5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a598c19-d4e5-469f-b7c3-aec08505e234",
   "metadata": {},
   "source": [
    ">**F8.** Test how to select a cross-section of `df_timeseries` using the `\"MultiIndex\"` we set in **F2.**, specifically returning all the facilities in the state of Pennsylvania (`\"PA\"`). Then transpose the test `DataFrame`, as the format convenient for plotting time series line graphs.\n",
    ">```\n",
    "df_timeseries.xs(\"PA\", level=\"STATE\").T\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5c5ff8-b3ee-4ecd-8f04-df57c31934ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "978fc8c1-6abf-4b1d-b680-5cb526095dea",
   "metadata": {},
   "source": [
    "<font color=\"green\">***F9. Prep** (OPTIONAL)*   \n",
    "*- The final statistical graph consists of 12 subplots. They are arranged over 4 rows and 3 columns.*        \n",
    "*- We will therefore be setting up 12 axes instances in this plotting routine, not just 1 as in D6.*    \n",
    "*- Note that the `axes` variable is the equivalent of specifying the 12 axes instances as a 2D numpy array of size 4x3:*\n",
    "```\n",
    "fig, [[ax0, ax1, ax2], [ax3, ax4, ax5], [ax6, ax7, ax8], [ax9, ax10, ax11]] = plt.subplots(nrows=4, ncols=3)\n",
    "```   \n",
    "<font color=\"green\">or conceptually:\n",
    "```\n",
    "array([[\"ax0\", \"ax1\", \"ax2\"],\n",
    "       [\"ax3\", \"ax4\", \"ax5\"],\n",
    "       [\"ax6\", \"ax7\", \"ax8\"],\n",
    "       [\"ax9\", \"ax10\", \"ax11\"]])\n",
    "```\n",
    "<font color=\"green\">When we `flatten()` `axes` (to `zip` it for the `for` loop) this collapses the array into 1 dimension, i.e.:\n",
    "```\n",
    "np.array([ax0, ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8, ax9, ax10, ax11])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316dd115-cee4-4ada-8196-482a7899f7af",
   "metadata": {
    "tags": []
   },
   "source": [
    ">**F9.** Make the final statistical graph. Use matplotlib to set-up and generate state-level time series subplots showing the 2011-2021 emissions for each facility in the 12 states with the worst aggregate cumulative emissions.\n",
    "> \n",
    ">**Code Detail:** The central operation is the `for` loop that gets the state-specific `df_timeseries` cross-section, transposes and plots on the designated plotting area (axes). Note the many keyword argument used and available to fine-tune the plot aesthetics and readability.\n",
    ">\n",
    ">**Tech Note:** The computation will again take a moment!\n",
    ">```\n",
    "fig, axes = plt.subplots(nrows=4, ncols=3, figsize=(25,20), sharey=True)\n",
    "for state, ax in zip(worst12states, axes.flatten()):\n",
    "    df_timeseries.xs(state, level=\"STATE\").T.plot(ax=ax)\n",
    "    ax.set_title(state)\n",
    "    ax.legend(loc=\"best\", fontsize=\"medium\", labelspacing=0.25)\n",
    ">\n",
    ">plt.setp(axes, xlabel=\"Reporting Year\", ylabel=\"CO2e metric tons per Reporting Year\", xticks=range(len(df_timeseries.columns)), xticklabels= df_timeseries.columns,    # range(2011, 2022) range(0,11)\n",
    "         yticks=range(0,550001,25000))\n",
    "#plt.tight_layout()\n",
    "#plt.savefig(\"<SomeFilename>.png\", dpi=600)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0e1e49-80fd-4586-9a9f-7783633c45e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bca36e2b-0f28-408d-b708-115874b4aa66",
   "metadata": {},
   "source": [
    "<font color=\"green\">***F9. Comment***        \n",
    "*- Different approaches to Carbon Reporting and accountability could reveal different insights. Instead of the EPA's GHGRP requiring individual facilities exceeding the 25,000 CO 2 e metric tons threshold to report their pollution, there may be more interesting trends if this requirements was at Parent-level.*      \n",
    "*- Whilst we know all the individual large facilities with the same parent university (e.g. Yale's 2 large facilities, School of Medicine and Central Power Plant), we don't know if the sum of these large facilities equates to the full picture of the parent university's carbon pollution (e.g. as there is no visibility of any smaller facilities of Yale's which are not required to report.* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a585577a-60a5-4f4d-abcf-76668c46d787",
   "metadata": {},
   "source": [
    "---\n",
    "Copyright © 2023 Rho Zeta AI Ltd. All rights reserved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "nteract": {
   "version": "0.28.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
